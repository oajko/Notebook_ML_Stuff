{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Dense, Dropout, LayerNormalization, Concatenate, Input, Flatten\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "tf.keras.utils.set_random_seed(812)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gate(Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.linear = Dense(units, activation = 'linear')\n",
    "        self.sigmoid = Dense(units, activation = 'sigmoid')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.sigmoid(inputs) * self.linear(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRN(Layer):\n",
    "    def __init__(self, units, drop, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.elu = Dense(units, activation = 'elu')\n",
    "        self.linear = Dense(units, activation = 'linear')\n",
    "        self.drop = Dropout(drop)\n",
    "        self.gate = Gate(units)\n",
    "        self.layer_norm = LayerNormalization()\n",
    "        self.filter = Dense(units, activation = 'linear')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.elu(inputs)\n",
    "        x = self.linear(x)\n",
    "        x = self.drop(x)\n",
    "        if inputs.shape[-1] != self.units:\n",
    "            inputs = self.filter(inputs)\n",
    "        x = self.gate(x)\n",
    "        x = x + inputs\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate into own GRU layer. And dot product with total GRN.\n",
    "\n",
    "class VariableSelection(Layer):\n",
    "    def __init__(self, units, drop, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.gru = GRN(units, drop)\n",
    "        self.softmax = Dense(units, activation = 'softmax')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        feat_list = list()\n",
    "        for i in tf.unstack(inputs, axis = 1):\n",
    "            x = self.gru(i)\n",
    "            feat_list.append(x)\n",
    "        individual_gru = Concatenate()(feat_list)\n",
    "        individual_gru = tf.expand_dims(self.softmax(individual_gru), axis = -1)\n",
    "        total = self.gru(inputs)\n",
    "        output = tf.linalg.matmul(total, individual_gru)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_selection(df):\n",
    "    start = Input(shape = (df.shape[1], 1))\n",
    "    x = VariableSelection(64, 0.6)(start)\n",
    "    x = VariableSelection(32, 0.2)(start)\n",
    "    x = VariableSelection(16, 0.2)(x)\n",
    "    x = Flatten()(x)\n",
    "    end = Dense(units = 1, activation = 'sigmoid')(x)\n",
    "    model = tf.keras.Model(start, end)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_model(df):\n",
    "    start = Input(shape = (df.shape[1], 1))\n",
    "    x = Flatten()(start)\n",
    "    end = Dense(units = 1, activation = 'sigmoid')(x)\n",
    "    model = tf.keras.Model(start, end)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('titanic.csv').select_dtypes(exclude = object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6493799090385437\n"
     ]
    }
   ],
   "source": [
    "mdl = variable_selection(df.drop('Survived', axis = 1))\n",
    "mdl.compile(loss = 'binary_crossentropy', optimizer = 'SGD', metrics = ['Accuracy'])\n",
    "history = mdl.fit(x = df.iloc[:, 1:], y = df.Survived, epochs = 30, verbose = 0)\n",
    "print(max(history.history['Accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5873731970787048\n"
     ]
    }
   ],
   "source": [
    "mdl = base_model(df.drop('Survived', axis = 1))\n",
    "mdl.compile(loss = 'binary_crossentropy', optimizer = 'SGD', metrics = ['Accuracy'])\n",
    "history = mdl.fit(x = df.iloc[:, 1:], y = df.Survived, epochs = 30, verbose = 0)\n",
    "print(max(history.history['Accuracy']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNN testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_dnn(df):\n",
    "    start = Input(shape = (df.shape[1], 1))\n",
    "    x = VariableSelection(64, 0.2)(start)\n",
    "    x = VariableSelection(32, 0.2)(x)\n",
    "    x = VariableSelection(16, 0.2)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64, activation = 'relu')(x)\n",
    "    x = Dense(32, activation = 'relu')(x)\n",
    "    end = Dense(units = 1, activation = 'sigmoid')(x)\n",
    "    model = tf.keras.Model(start, end)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_dnn(df):\n",
    "    start = Input(shape = (df.shape[1], 1))\n",
    "    x = Flatten()(start)\n",
    "    x = Dense(64, activation = 'relu')(x)\n",
    "    x = Dense(32, activation = 'relu')(x)\n",
    "    end = Dense(units = 1, activation = 'sigmoid')(x)\n",
    "    model = tf.keras.Model(start, end)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6324689984321594\n"
     ]
    }
   ],
   "source": [
    "mdl = variable_dnn(df.drop('Survived', axis = 1))\n",
    "mdl.compile(loss = 'binary_crossentropy', optimizer = 'SGD', metrics = ['Accuracy'])\n",
    "history = mdl.fit(x = df.iloc[:, 1:], y = df.Survived, epochs = 30, verbose = 0)\n",
    "print(max(history.history['Accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7113866806030273\n"
     ]
    }
   ],
   "source": [
    "mdl = base_dnn(df.drop('Survived', axis = 1))\n",
    "mdl.compile(loss = 'binary_crossentropy', optimizer = 'SGD', metrics = ['Accuracy'])\n",
    "history = mdl.fit(x = df.iloc[:, 1:], y = df.Survived, epochs = 30, verbose = 0)\n",
    "print(max(history.history['Accuracy']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalization ability using TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, 1:], df.Survived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6216216216216216"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl = variable_selection(df.drop('Survived', axis = 1))\n",
    "mdl.compile(loss = 'binary_crossentropy', optimizer = 'SGD', metrics = ['Accuracy'])\n",
    "mdl.fit(x = X_train, y = y_train, epochs = 30, verbose = 0)\n",
    "preds = mdl.predict(X_test, verbose = 0)\n",
    "(np.round(preds).flatten() == y_test).sum()/ len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6846846846846847"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl = base_model(df.drop('Survived', axis = 1))\n",
    "mdl.compile(loss = 'binary_crossentropy', optimizer = 'SGD', metrics = ['Accuracy'])\n",
    "mdl.fit(x = X_train, y = y_train, epochs = 30, verbose = 0)\n",
    "preds = mdl.predict(X_test, verbose = 0)\n",
    "(np.round(preds).flatten() == y_test).sum()/ len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the results above is trained on the titanic dataset. We test variable selection model on three tests. No hidden network (input of (5, 1) -> output (1, )) network, a dnn network with two hidden layers with 64 and 32 nodes, then finally testing generalization ability.\n",
    "\n",
    "With no hidden layers, variable selection network (VSN) is significantly stronger than the base model by roughly 0.06 (6%) points higher. However, VSN was worse in both DNN and generalization by roughly 0.06 in the opposite direction.\n",
    "\n",
    "Note that the titanic dataset is small, and neural networks, in general, should not be used for this dataset. VSN also is generally better for noisy/ many feature datasets, which this clearly is not with 5 features. And so, the findings of this notebook just shows VSN is not good for a relatively clean and easy dataset (you can get 85% on this dataset with tree networks). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
